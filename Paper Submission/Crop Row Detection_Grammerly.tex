\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{subcaption} % For subfigures and captions
\usepackage{caption}
\captionsetup[figure]{labelformat=simple, labelsep=period}


\title{Waypoint Generation based on Crop Row Detection Using Aerial Imaging Systems}

\author{
\IEEEauthorblockN{Alireza Amiri}
\IEEEauthorblockA{Department of  Mechatronics Engineering\\
K N. Toosi University of Technology\\
Tehran, Iran\\
ali.amiri@email.kntu.ac.ir}
\and
\IEEEauthorblockN{Saeed Khankalantary}
\IEEEauthorblockA{Department of Mechatronics Engineering\\
K N. Toosi University of Technology\\
Tehran, Iran\\
s.kalantary@kntu.ac.ir}
}

\begin{document}

\maketitle

\begin{abstract}
In autonomous agriculture, using wheeled mobile robots for various tasks requires the precise generation of waypoints to define the robots' paths accurately. This article presents a method that utilizes aerial imagery to detect crop positions and establish waypoints. A specialized hardware setup comprising a high-resolution camera, wireless transmitter, and receiver has been developed to capture and transmit live images of the agricultural field. In the image processing stage, crops are identified using two parallel techniques—Unet and K-means clustering. Subsequently, the integration of linear regression and Hough Transform is examined to detect crop row lines, refined through filtering to ensure that a single, accurate line represents each row. Finally, specific points on the paths between these rows are selected and converted into global coordinates, enabling the system to support real-time crop detection and precise waypoint generation, thus facilitating autonomous navigation for agricultural robots.
\end{abstract}

\begin{IEEEkeywords}
Crop Row Detection, Hough Transform, Image Processing
\end{IEEEkeywords}

\section{Introduction}
Integrating advanced technologies, such as artificial intelligence, sensing systems, and autonomous robots, is crucial in addressing global food challenges by enhancing agricultural productivity and sustainability \cite{b2,b3}. Precision Agriculture (PA) has emerged as an intelligent management system that optimizes input distribution, such as water and fertilizers, based on site-specific needs, thereby improving crop yield and resource-use efficiency while minimizing environmental impact \cite{b5,b6}.
Mobile robots in precision agriculture have been utilized in various field tasks, such as fertilization, irrigation, weeding, harvesting, and crop picking \cite{b2,b3}.

In precision agriculture, traditional GPS-based path planning for agricultural machinery remains common. However, it presents challenges, such as the risk of seedling injury due to deviations between the ideal path and the actual crop rows \cite{b1}. To address these issues, machine vision-based crop row detection on unmanned agricultural machinery has gained attention, allowing for real-time, precise path planning that minimizes crop damage \cite{b1,b8}. However, the unstructured agricultural environment complicates accurate navigation and autonomous operations, necessitating the integration of onboard sensors, such as scanning lasers and machine vision cameras, to enhance the robot's ability to sense and interact with its surroundings \cite{b2,b3}. Despite these advancements, ground-based platforms face challenges, including soil compaction and vibrations from uneven terrain, which can be mitigated by utilizing UAVs for high-resolution, low-altitude aerial sensing \cite{b10}.

Unmanned Aerial Vehicles (UAVs) have increasingly become a vital tool in precision agriculture, offering a flexible, cost-effective platform for high-resolution remote sensing  \cite{b9,b12}. UAVs can capture detailed imagery under different conditions, providing satisfactory spatial resolutions and covering significant areas, essential for monitoring crop variability and supporting temporal analysis \cite{b10,b12}. These platforms excel in vegetation segmentation, weed management, and crop row detection applications, filling the gap between terrestrial and satellite-based remote sensing \cite{b7,b13}. Despite certain limitations, such as flight endurance, the ability to conduct self-automated flights and provide timely data collection makes UAVs indispensable in modern agriculture \cite{b11,b13}. However, aerial image post-processing is necessary to differentiate crop rows from soil and weeds, highlighting the complexity of their integration in precision farming \cite{b6}.
The initial step in crop row detection is semantic segmentation of aerial images to determine the positions where vegetation exists. Previous works experimented with different approaches to achieve a trusted and accurate segmentation model.


The literature on crop row detection encompasses a range of approaches, each employing distinct methodologies to enhance precision in agricultural applications. Vegetation indices (VIs) like NDVI, ExG, and SAVI are frequently utilized as inputs for various detection methods, including thresholding algorithms, K-means clustering, and the Minimum Distance to the Mean (MDM) classifier. These methods are instrumental in segmenting vegetation from soil backgrounds and effectively identifying crop rows \cite{b1,b6,b13}. Fusion approaches, which combine RGB and NDVI data, are also explored to improve segmentation processes, particularly for autonomous robotic navigation in agricultural fields \cite{b5}.

AI-based approaches have gained prominence, particularly deep learning models, which have significantly improved detection accuracy. For instance, CRowNet, which combines a convolutional neural network (CNN) with the Hough transform, demonstrates robust detection capabilities across various crop types and field conditions, achieving high detection rates even in complex scenarios like curved or intersecting rows \cite{b8,b14}. Additionally, networks like U-Net, SegNet, and ModSegNet are used with traditional and deep learning-based semantic segmentation methods. These networks have shown varying degrees of effectiveness, with deep learning models generally outperforming classical approaches, particularly in challenging conditions where traditional methods may falter \cite{b5,b13}.

Traditional computer vision techniques are widely employed in crop row detection, with the Hough Transform being a prevalent method for line detection \cite{b2,b15}. Despite its extensive use, the Hough Transform has inherent limitations, such as high computational complexity and sensitivity to noise, which compromise its suitability for real-time applications \cite{b2}. To address these issues, various adaptations, including the Probabilistic and Multi-scale Hough Transforms, have been developed \cite{b2}. Moreover, combining the Hough Transform with deep learning has been shown to improve detection accuracy \cite{b8}. Linear regression is another commonly used technique, valued for its simplicity and computational efficiency, especially when integrated with preprocessing steps like image segmentation and feature extraction \cite{b2,b3}. Other methods, such as the Horizontal Strips Method and Blob Analysis, are also utilized in crop row detection. The Horizontal Strips Method enhances computational efficiency by bypassing additional segmentation steps but may suffer from accuracy issues due to factors like camera angle and missing rows \cite{b2}. Blob Analysis, which groups connected pixels into blobs to generate crop rows, can struggle in environments with high weed density \cite{b2}. The Random Sample Consensus algorithm is another approach that provides robust row detection by estimating mathematical models from data with outliers. However, its effectiveness depends on factors such as the quality of extracted feature points \cite{b2}. Machine learning methods, including clustering techniques like K-means and deep learning models such as Faster R-CNN, YOLOv3, and SegNet, are increasingly being applied in this domain \cite{b2,b5}. These methods offer significant advantages, although challenges remain, particularly in handling varying field conditions and limited annotated data \cite{b2,b5}.

In precision agriculture, extracted crop row and inter-row information serves as a foundation for various tasks, such as guiding autonomous ground vehicles by defining navigation paths between rows, which are crucial for tasks like automatic path computation and the development of vigor maps for field partitioning \cite{b11}. For aerial images, the navigation path is identified as the line between two crop rows, while for unmanned agricultural vehicles, it is defined by the central angle between two rows\cite{b1}.


This paper presents a novel system for precise navigation in autonomous agricultural robots through real-time aerial image acquisition and waypoint generation. The integrated hardware-software solution uses a UAV-mounted camera and a radio transmission system to capture and relay high-resolution RGB images and GPS coordinates. The system's core is an advanced image processing algorithm that divides aerial images into sub-images and employs two segmentation methods—color filtering and a U-Net model—for crop identification. Crop row detection is refined using linear regression and the Hough Transform, while waypoints are generated by mapping local pixel data to global GPS coordinates. This integrated approach improves the navigation capabilities of agricultural robots in complex field environments.

The paper is structured as follows. Section \ref{Data Acquisition} introduces a hardware setup for aerial imagery data acquisition. Section \ref{Image Processing} explains the image processing procedure, including image splitting, applying two methods—K-means clustering and the U-Net model—for image segmentation, crop row detection using linear regression and Hough Transform, and the waypoint generation method. The section concludes with the reconstruction of split images. Section \ref{Image Coordination to Global Coordination Conversion} describes converting the local position of waypoints in previous sections to the global coordinate system. The experimental setup of the proposed system is presented in the Experiments section. Conclusions and further improvements are finally discussed in the concluding section
\ref{Conclusion}.


\section{Data Acquisition}\label{Data Acquisition}
In response to the need for live waypoint generation in agricultural fields, a hardware setup is required to provide the necessary environmental information. This system captures and transmits data to the software unit, where processing occurs. In the context of this project, a lightweight camera is crucial to take high-resolution aerial images of the target field, which will be the main data source for the image processing algorithm. Several cameras including Sequoia
\cite{b9,b4,b7,b6}
, Micasense RedEdge
\cite{b9,b14}, and DJI Zenmuse X7
\cite{b5} were utilized in similar projects. Considering the specifications of these cameras and crop row detection algorithms developed based on their aerial images, it was concluded that having high-resolution RGB images meets the needs of this project. Although the NIR band contains valuable information about the field and can be used for vegetation segmentation, they are too sensitive to environmental conditions such as temperature. They might lead to the poor performance of the software
\cite{b5}. In conclusion, the Gopro HERO4 camera was acquired for this project. The specifications of this camera, including its lightweight, high-quality RGB images and durability, make it a suitable choice for aerial imagery. Figure
\ref{gopro} shows the camera used in this research.
Besides the camera, a radio transmission system consisting of power supplies, a transmitter, and a receiver was developed to provide live data transfer between the mounted camera on the UAV and the computer with an image processing program.
Also, the last step of this project, converting pixel coordination on the images to global coordination, requires the global coordination of the camera at the time of photo capturing as a reference. A Ublox-Neo-6m GPS module must be mounted on the camera UAV to provide this information.

\begin{figure}[h]
\includegraphics[width=\linewidth]{GoPro.jpg}
\caption{GoPro camera used in this paper}
\label{gopro}
\end{figure}


\section{Image Processing}\label{Image Processing}
The main contribution of this paper lies in this section, which is to develop an image-processing pipeline to determine the position of pixels in the path between the crops. Figure
\ref{fig_procedure}
shows the procedure proposed in this paper. Initially, the aerial images captured with the camera are split into sub-images. Then, during a semantic segmentation process, a binary mask is generated for each sub-image showing the positions of vegetation and background. In the next step, using Hough Transform, crop rows are detected. After defining the path line, a desired number of equally spaced points are selected on this line, and their position is recorded for later steps. Finally, the sub-images and their local waypoint coordinates are reconstructed, forming the original input image.

\begin{figure}[htbp]
\includegraphics[width=\linewidth]{Block Diagram.png}
\caption{procedure of waypoint generation using aerial images
\cite{b5}}
\label{fig_procedure}
\end{figure}

\subsection{Preprocessing}\label{Preprocessing}
High-resolution aerial images covering large areas are typically too large to be used directly in image processing algorithms and networks.
Besides, due to the variation in terrain topologies, the non-uniform shape of crop rows -including curved or irregular shapes- is common in some agricultural fields
\cite{b2, b3, b14}.
Figure \ref{fig_Aerial} shows an example of aerial images used in similar projects.
Given this information, calculating a straight line for an entire crop row in the original image is not an accurate approach for crop row detection.
To address this issue, the aerial image is initially split into equal-sized sections with a static image size of 512 by 512 pixels. These sub-images are named accordingly and will be reconstructed after the image-processing procedure. Their chosen size ensures compatibility with the algorithms and networks designed in the later steps of the image-processing section.


\begin{figure}[htbp]
\includegraphics[width=\linewidth]{Aerial Image2.png}
\caption{Aerial image sample used for waypoint detection
\cite{b5}}
\label{fig_Aerial}
\end{figure}

\subsection{Image Segmentation}\label{Image Segmentation}
image segmentation is the most computationally intensive section of this project, while its outcome directly impacts the system's overall accuracy. Given the RGB sub-images, it is important to identify the exact locations where crops exist, which is done by processing the image and generation of a binary mask. The following section introduces and implements two semantic segmentation methods, and their results are assessed.

\paragraph{Color-Filtering Based Image Segmentation}
The initial idea for image segmentation of crops in agricultural fields was to apply color filtering methods to isolate the areas having a green color. This method was first used by analyzing the image's green channel but performed poorly for a few reasons. First, the green channel filtering method cannot filter white pixels since they also include a large amount of green color. Second, there are sections of crops with color tending to yellow, which were unintentionally removed from the layer mask. The two issues mentioned were later resolved using a prefiltering for white pixels removal and also by converting the RGB image to HSV format (Hue, Saturation, and Value) so that it would be possible to filter the pixels based on their color considering their Vegetation Index (VI).

Next, a K-means clustering method was applied to the color-based filtered image to highlight the areas, including crops. The clustering helped remove sparse pixels identified as crops and unite the areas where crops exist. The Parameters of the K-means algorithm were defined in an iterative approach to determine the best-performing setting,

Without a high computational cost, this method can generate an accurate binary mask indicating the areas where vegetation exists. However, examining this method on the images taken from different fields with various crops, it was observed that it fails when unwanted vegetation like grass and weeds exists. Since the model filters the pixels based on their color, it cannot isolate crops without including the other vegetation. The same results in
\cite{b5} are observed.
Figure
\ref{fig2:kmeans} shows the performance of this method on different sub-images.


\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{kmeans_predictions.png}
\caption{Color-Filtering Based Image Segmentation
\cite{b5}}
\label{fig2:kmeans}
\end{figure}


\paragraph{Unet-Based Image Segmentation}
The limited performance of color-based image segmentation in crop fields indicates the need for an intelligent model that can recognize the crops using more complicated methods. Utilizing machine learning methods to train semantic segmentation models has proven to be a reliable solution for crop detection. In this approach, a Unet network was chosen as the primary image segmentation tool (Fig
\ref{fig_Unet})
and was trained on a dataset developed by
\cite{b5}
. This dataset consists of UAV-based aerial images taken from three vineyards with corresponding ground truth images, indicating the location of vines in the vineyard.

The original and ground truth images were initially split into sub-images using a sliding window of size 512 by 512 pixels to use the annotated aerial images. The sliding step size was chosen to be 50 pixels, which resulted in more sample data for training. The resulting dataset consists of 5089 sub-images, which are divided into train, test, and validation sets with a ratio of 80\%\
, 10\%\
, 10\%\
.

The Unet model was trained in 50 epochs, which reached the early stopping condition at epoch 7 with an accuracy of 96\%\ and a loss value of 0.09. The predictions of this model on test data prove its ability to semantically segment crops in different conditions, even in the presence of unwanted vegetation (Fig. 
\ref{fig2:Unet_Segmentation})


\begin{figure}[htbp]
\includegraphics[width=\linewidth]{UNET.png}
\caption{Unet model architecture}
\label{fig_Unet}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{unet_predictions.png}
\caption{Unet-Based Image Segmentation
\cite{b5}}
\label{fig2:Unet_Segmentation}
\end{figure}

\subsection{Crop Row Detection}\label{Crop Row Detection}
The crop row detection process aims to assign a straight line with a defined slope and intercept each group of white pixels annotated as crops in the binary mask. This study analyzed and implemented two approaches—linear regression and the Hough Transform—for this purpose.

\subsubsection{Linear Regression}\label{Linear Regression}
This experiment used the binary layer mask of segmented crops to assign each pixel to its corresponding crop row. The first step involved determining the optimal angle of the crop rows through an iterative algorithm, assuming that all rows are parallel and share the same slope. Once the slope was defined, the x and y coordinates of the white pixels were rotated to align the crop rows vertically, simplifying the clustering process.

At this stage, the K-means algorithm was applied with a customized loss function designed to minimize the least square error of the distances between the white pixels and a vertical line. Unlike standard loss functions that measure the distance from a centroid point, this approach considered each cluster centered around a line, not a point.

Given the variability in the number of clusters across different images, the clustering algorithm is initiated by grouping nearby pixels. When a pixel was too distant from the existing group, it was treated as the center of a new cluster. However, this approach led to the formation of numerous unwanted clusters. To address this, adjacent clusters were merged in a subsequent step. Finally, the initial rotation of the pixel coordinates was reversed to visualize the resulting clusters. As illustrated in the results, even after merging close clusters, the method failed to accurately identify crop rows, leading to complications in the later stages of the project. To conclude the experiment, linear regression was applied to fit a line to the pixels within each cluster. However, the results indicated that this clustering and linear regression approach is ineffective for defining crop rows, necessitating exploring alternative methods.

\subsubsection{Hough Transform}\label{Hough Transform}
Another widely used solution for crop row detection is the Hough Transform. This computer vision technique is primarily designed to identify geometric shapes in images, making it particularly effective for detecting lines and curves. In this project, the Hough Transform was employed as an alternative to the linear regression model for predicting crop rows. Implementing this method requires significantly less preprocessing and image manipulation, mainly due to the availability of related packages in OpenCV. When the Hough Transform was applied to the images, multiple lines were detected for each crop row, as illustrated in Fig.
\ref{fig_Hough_Init}. These lines successfully covered the crop rows, indicating the method's effectiveness in line detection. To refine the results and assign only one line per crop row, the detected lines were merged, and a single line was calculated using the average slope and intercept of the detected lines. The final results are presented in Fig.
\ref{fig_Hough_mod}.

\begin{figure}[htbp]
\centering
\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=0.7\linewidth]{Hough initial2.png}
\caption{Initial Hough transform implementation}
\label{fig_Hough_Init}
\end{subfigure}

\vspace{0.5cm} % Adjust the space between the two subfigures as needed

\begin{subfigure}{\linewidth}
\centering
\includegraphics[width=0.7\linewidth]{Hough Revised2.png}
\caption{Modified Hough transform implementation}
\label{fig_Hough_mod}
\end{subfigure}

\caption{Crop row detection methods using Hough transform \cite{b5}}
\label{fig:comparison}
\end{figure}


\subsection{Path Waypoint Detection}\label{Path Waypoint Detection}
As stated in previous steps, the most computational parts of this study are image segmentation and crop row detection. Determining crop rows as lines with defined slope and intercept makes calculating a line indicating the path straightforward. The path is defined as a line parallel to two neighboring crop rows at an equal distance from each other.
Waypoints can be defined by choosing a specific number of equally spaced points along each path. These waypoints' local coordination is stored for later study steps.

\subsection{Reconstruction of Aerial Image}\label{Reconstruction of Aerial Image}
Following the image splitting described in section
\ref{Preprocessing}, the image processing tasks outlined earlier will be applied to each segment of the leading aerial image. The goal is to determine and record the waypoint coordinates within each segment. Once the waypoint coordinates are computed, the initial image must be reconstructed and assembled. Subsequently, the waypoint coordinates need to be converted to align with the coordinates of the newly assembled image.

\section{Image Coordination to Global Coordination Conversion}\label{Image Coordination to Global Coordination Conversion}
An analytical approach and mathematical algorithm are required to obtain the corresponding global coordinates of the points identified in the images. This process involves using the global coordinates of the camera at the time of image capture, along with the camera's height. The conversion process is broken down into two phases: first, converting pixel coordinates into meter distances, and second, converting these meter distances into the global coordinate system.

\subsection{Pixel to Meter Conversion}\label{Pixel to Meter Conversion}
Assuming the camera is positioned at the center of the image at a height of \( h \), and given the angle \( \theta \) (defined as the angle between the standard line from the camera to the ground and the line connecting the camera to the edge of the image), the conversion from pixel position to ground distance is described by the following equations:
\[
\tan \theta = \frac{x}{h} \implies x = \tan \theta  \cdot h
\]
Where \( x \) is the distance from the camera to the edge of the image in meters.

If the image length in pixels is denoted as \( L \), then:
\[
1 \text{ (pixel)} = 2h \cdot \frac{\tan \theta}{L} \text{ (m)}
\]

Using these equations, the position of a pixel with coordinations \((x, y)\) in the image can be determined relative to the camera's position on the ground.

\subsection{Meter to Global Coordination Conversion}\label{Meter to Global Coordination Conversion}
The final step involves converting the meter distances obtained from the previous calculations into global coordinates. The relationship between meters and degrees of latitude or longitude is given by:
\[
1 \text{ (m)} = 0.00001^\circ
\]

This conversion factor enables the translation of local pixel positions, measured in meters, into global coordinates.

\section{Experimental Setup}\label{Experimental Setup}
In alignment with the primary objective of this research, which aimed to develop an integrated system for acquiring live aerial imagery and calculating global coordinates for path waypoints, an experimental setup incorporating a camera and radio transmission system was designed and implemented. Using the RC805 radio transmission system, a wireless connection between the camera and a computer was established to facilitate the transfer of live images captured by a GoPro Hero4 camera. The experimental configuration is illustrated in Fig
\ref{EXP}.

The previously described image processing features were consolidated into a program with a graphical user interface (GUI). This system allows for two modes of operation: offline, where pre-existing images stored locally on the computer are loaded, and online, where live-stream imagery from the camera is received. The results of each image processing step are subsequently displayed in the program's output, providing a visual representation of the logic underpinning the generated waypoints.

\begin{figure}[htbp]
\includegraphics[width=\linewidth]{Setup of experiment.png}
\caption{Experimental setup of proposed waypoint generation system}
\label{EXP}
\end{figure}


\section{Conclusion}\label{Conclusion}
The paper addresses the growing need for precise navigation systems utilized by autonomous agricultural mobile robots. It presents a hardware setup and an image processing procedure for acquiring live aerial images of agricultural fields and identifying the global coordinates of the waypoints placed within the field. These coordinates serve as reference points for mobile robot navigation systems. The environmental information of the field is gathered using high-resolution RGB aerial images acquired by cameras mounted on UAVs, along with the global coordinates of the UAV acquired by GPS modules. A radio transmission system facilitates data transfer from the UAV position to the processing unit. The procedure for extracting the global coordinates of the path from the acquired aerial images is defined as an image processing algorithm encompassing image segmentation, crop row detection, and waypoint generation.

Initially, the aerial image is divided into equal-sized sub-images to ensure uniform inputs for the image processing algorithms. Two image segmentation methods, namely color-filtering-based and Unet-based, are considered to identify the position of the crops in the resulting sub-images. The study demonstrates that while both methods can perform image segmentation tasks, the color filtering approach is ineffective in fields with other unwanted vegetation, such as grass and weeds. Conversely, the trained Unet model achieves an accuracy of 96\%\ and generates satisfactory predictions of plant positions in different fields. Consequently, the Unet-based method is selected as the primary image segmentation tool.

Following the segmentation step, linear regression and Hough Transform methods are evaluated for crop row detection. The analysis reveals that the linear regression method's complexity, encompassing pixel clustering and line fitting, renders it a cumbersome solution for this task, leading to unsatisfactory crop row detections. In contrast, implementing Hough Transform as an alternative crop row detection approach results in more accurate estimated lines. Waypoints are defined as equally spaced points on the path, calculated as parallel lines between two crop rows.

The global coordinates of the defined waypoints are obtained through the reconstruction and mapping of the local coordinates found in each sub-image to the local coordinates of the original image. Subsequently, the global coordinates are calculated based on the local position of pixels and the global coordinates of the camera at the time of image capture and stored for further utilization.


\begin{thebibliography}{00}
\bibitem{b1} Y. Yang et al., "Real-time detection of crop rows in maize fields based on autonomous extraction of ROI," Expert Systems with Applications, vol. 213, p. 118826, 2023.
\bibitem{b2} J. Shi, Y. Bai, Z. Diao, J. Zhou, X. Yao, and B. Zhang, "Row detection BASED navigation and guidance for agricultural robots and autonomous vehicles in row-crop fields: methods and applications," Agronomy, vol. 13, no. 7, p. 1780, 2023.
\bibitem{b3} V. R. Ponnambalam, M. Bakken, R. J. Moore, J. Glenn Omholt Gjevestad, and P. Johan From, "Autonomous crop row guidance using adaptive multi-roi in strawberry fields," Sensors, vol. 20, no. 18, p. 5249, 2020.
\bibitem{b4} N. Cunha, T. Barros, M. Reis, T. Marta, C. Premebida, and U. J. Nunes, "Multispectral image segmentation in agriculture: A comprehensive study on fusion approaches," in Iberian Robotics conference, 2023: Springer, pp. 311-323.
\bibitem{b5} T. Barros et al., "Multispectral vineyard segmentation: A deep learning comparison study," Computers and electronics in agriculture, vol. 195, p. 106782, 2022.
\bibitem{b6} G. Ronchetti, A. Mayer, A. Facchi, B. Ortuani, and G. Sona, "Crop row detection through UAV surveys to optimize on-farm irrigation management," Remote Sensing, vol. 12, no. 12, p. 1967, 2020.
\bibitem{b7} M. Hassanein, M. Khedr, and N. El-Sheimy, "Crop row detection procedure using low-cost UAV imagery system," The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, vol. 42, pp. 349-356, 2019.
\bibitem{b8} M. D. Bah, A. Hafiane, and R. Canals, "CRowNet: Deep network for crop row detection in UAV images," IEEE Access, vol. 8, pp. 5189-5200, 2019.
\bibitem{b9} I. Sa et al., "WeedMap: A large-scale semantic weed mapping framework using aerial multispectral imaging and deep neural network for precision farming," Remote Sensing, vol. 10, no. 9, p. 1423, 2018.
\bibitem{b10} S. Sankaran et al., "Low-altitude, high-resolution aerial imaging systems for row and field crop phenotyping: A review," European Journal of Agronomy, vol. 70, pp. 112-123, 2015.
\bibitem{b11} L. Comba, P. Gay, J. Primicerio, and D. R. Aimonino, "Vineyard detection from unmanned aerial systems images," Computers and Electronics in Agriculture, vol. 114, pp. 78-87, 2015.
\bibitem{b12} K. Ramesh, N. Chandrika, S. Omkar, M. Meenavathi, and V. Rekha, "Detection of rows in agricultural crop images acquired by remote sensing from a UAV," International Journal of Image, Graphics and Signal Processing, vol. 8, no. 11, p. 25, 2016.
\bibitem{b13} M. Pérez-Ortiz, J. Peña, P. A. Gutiérrez, J. Torres-Sánchez, C. Hervás-Martínez, and F. López-Granados, "A semi-supervised system for weed mapping in sunflower crops using unmanned aerial vehicles and a crop row detection method," Applied Soft Computing, vol. 37, pp. 533-544, 2015.
\bibitem{b14} Y. Pang et al., "Improved crop row detection with deep neural network for early-season maize stand count in UAV imagery," Computers and Electronics in Agriculture, vol. 178, p. 105766, 2020.
\bibitem{b15} N. Samet, S. Hicsonmez, and E. Akbas, "Houghnet: Integrating near and long-range evidence for bottom-up object detection," in Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16, 2020: Springer, pp. 406-423.
\bibitem{b16} F.-A. Moreno, J. Monroy, J.-R. Ruiz-Sarmiento, C. Galindo, and J. Gonzalez-Jimenez, "Automatic waypoint generation to improve robot navigation through narrow spaces," Sensors, vol. 20, no. 1, p. 240, 2019.
\end{thebibliography}

\vspace{12pt}


\end{document}
